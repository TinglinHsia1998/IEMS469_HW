{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40995509-a5d3-4fbc-a271-71cd00f4cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Oct 21 16:34:31 2023\n",
    "\n",
    "@author: dinglin\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0eeaf61-713b-404a-a3fc-e689ec1de932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c74698-c339-4fd4-92f0-9090616a4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e50331-60e1-4aea-9a03-79619634f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess game frames\n",
    "def preprocess(frame):\n",
    "    frame = frame[35:195]\n",
    "    frame = frame[::2,::2,0]\n",
    "    frame[frame == 144] = 0\n",
    "    frame[frame == 109] = 0\n",
    "    frame[frame != 0] = 1\n",
    "    return np.reshape(frame.astype(np.float32), 6400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ae33b-cc7a-4a29-ba9e-22c54f9abeed",
   "metadata": {},
   "source": [
    "## Policy Network and Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af967438-5ba4-4a80-af7b-7c8f28fe8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # memories\n",
    "        self.ep_obs, self.ep_as, self.ep_rs, self.log_p = [], [], [], []\n",
    "        # parameters\n",
    "        self.gamma = 0.99\n",
    "        # neural networks\n",
    "        self.layer1 = nn.Linear(n_inputs, 200)\n",
    "        self.layer2 = nn.Linear(200, n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        # neural networks\n",
    "        self.layer1 = nn.Linear(n_inputs, 200)\n",
    "        self.layer2 = nn.Linear(200, 200)\n",
    "        self.output = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04899c70-7436-4d4d-8c80-563a6c9e3cad",
   "metadata": {},
   "source": [
    "## Policy Gradient with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43891c4b-9a77-4f5f-a826-f63a4b927c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient():\n",
    "    def __init__(self, n_inputs, n_outputs, lr_p, lr_v, gamma):\n",
    "        self.action_size = n_outputs\n",
    "        self.p_net = PolicyNetwork(n_inputs, n_outputs).to(device)\n",
    "        self.v_net = ValueNetwork(n_inputs).to(device)\n",
    "        self.a_optimizer = optim.Adam(self.p_net.parameters(), lr=lr_p)\n",
    "        self.v_optimizer = optim.Adam(self.v_net.parameters(), lr=lr_v)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        #get action probs then randomly sample from the probabilities\n",
    "        with torch.no_grad():\n",
    "            input_state = torch.FloatTensor(state).to(device)\n",
    "            action_probs = self.p_net(input_state)\n",
    "            #detach and turn to numpy to use with np.random.choice()\n",
    "            action_probs = action_probs.detach().cpu().numpy()\n",
    "            action = np.random.choice(np.arange(self.action_size), p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def train(self, state_list, action_list, reward_list):\n",
    "        \n",
    "        #turn rewards into return\n",
    "        trajectory_len = len(reward_list)\n",
    "        return_array = np.zeros((trajectory_len,))\n",
    "        g_return = 0.\n",
    "        for i in range(trajectory_len-1,-1,-1):\n",
    "            g_return = reward_list[i] + self.gamma*g_return\n",
    "            return_array[i] = g_return\n",
    "            \n",
    "        # create tensors\n",
    "        state_t = torch.FloatTensor(state_list).to(device)\n",
    "        action_t = torch.LongTensor(action_list).to(device).view(-1,1)\n",
    "        return_t = torch.FloatTensor(return_array).to(device).view(-1,1)\n",
    "        \n",
    "        # get value function estimates\n",
    "        vf_t = self.v_net(state_t).to(device)\n",
    "        with torch.no_grad():\n",
    "            advantage_t = return_t - vf_t\n",
    "        \n",
    "        # calculate actor loss\n",
    "        selected_action_prob = self.p_net(state_t).gather(1, action_t)\n",
    "        # REINFORCE loss:\n",
    "        #actor_loss = torch.mean(-torch.log(selected_action_prob) * return_t)\n",
    "        # REINFORCE Baseline loss:\n",
    "        actor_loss = torch.mean(-torch.log(selected_action_prob) * advantage_t)\n",
    "        self.a_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.a_optimizer.step() \n",
    "\n",
    "        # calculate vf loss\n",
    "        loss_fn = nn.MSELoss()\n",
    "        vf_loss = loss_fn(vf_t, return_t)\n",
    "        self.v_optimizer.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.v_optimizer.step() \n",
    "        \n",
    "        return actor_loss.detach().cpu().numpy(), vf_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1dfd8-aed1-40b3-8914-18c45592f1dd",
   "metadata": {},
   "source": [
    "## Cart-Pole Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cf1997d-562e-4d27-bc4e-9a5780b0266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 20.0 Episode length: 0.0 Actor Loss: 0.5372 VF Loss: 7.5632\n",
      "Episode: 10 Total reward: 17.6 Episode length: 0.0 Actor Loss: 0.1848 VF Loss: 1.9552\n",
      "Episode: 20 Total reward: 15.7 Episode length: 0.0 Actor Loss: 0.2932 VF Loss: 3.8830\n",
      "Episode: 30 Total reward: 24.8 Episode length: 0.0 Actor Loss: 0.2076 VF Loss: 3.6657\n",
      "Episode: 40 Total reward: 13.4 Episode length: 0.0 Actor Loss: 0.0956 VF Loss: 1.5322\n",
      "Episode: 50 Total reward: 16.2 Episode length: 0.0 Actor Loss: 0.0236 VF Loss: 0.9113\n",
      "Episode: 60 Total reward: 21.3 Episode length: 0.0 Actor Loss: -0.1005 VF Loss: 0.5674\n",
      "Episode: 70 Total reward: 22.8 Episode length: 0.0 Actor Loss: -0.3741 VF Loss: 6.4060\n",
      "Episode: 80 Total reward: 20.8 Episode length: 0.0 Actor Loss: -0.2268 VF Loss: 3.6022\n",
      "Episode: 90 Total reward: 29.3 Episode length: 0.0 Actor Loss: 0.1115 VF Loss: 2.8257\n",
      "Episode: 100 Total reward: 36.1 Episode length: 0.0 Actor Loss: 0.1259 VF Loss: 1.9261\n",
      "Episode: 110 Total reward: 53.6 Episode length: 0.0 Actor Loss: 0.3486 VF Loss: 5.2448\n",
      "Episode: 120 Total reward: 35.0 Episode length: 0.0 Actor Loss: -0.2425 VF Loss: 2.8647\n",
      "Episode: 130 Total reward: 53.0 Episode length: 0.0 Actor Loss: 0.3207 VF Loss: 4.9976\n",
      "Episode: 140 Total reward: 70.8 Episode length: 0.0 Actor Loss: 0.0391 VF Loss: 2.2135\n",
      "Episode: 150 Total reward: 55.9 Episode length: 0.0 Actor Loss: -0.6878 VF Loss: 4.1841\n",
      "Episode: 160 Total reward: 73.9 Episode length: 0.0 Actor Loss: -0.1059 VF Loss: 2.1254\n",
      "Episode: 170 Total reward: 114.2 Episode length: 0.0 Actor Loss: 0.2404 VF Loss: 2.9189\n",
      "Episode: 180 Total reward: 123.1 Episode length: 0.0 Actor Loss: 0.0982 VF Loss: 1.7038\n",
      "Episode: 190 Total reward: 110.6 Episode length: 0.0 Actor Loss: -0.0500 VF Loss: 1.9202\n",
      "Episode: 200 Total reward: 124.4 Episode length: 0.0 Actor Loss: 0.0537 VF Loss: 1.6697\n",
      "Episode: 210 Total reward: 104.3 Episode length: 0.0 Actor Loss: 0.2941 VF Loss: 3.0908\n",
      "Episode: 220 Total reward: 145.4 Episode length: 0.0 Actor Loss: 0.0284 VF Loss: 1.3647\n",
      "Episode: 230 Total reward: 132.2 Episode length: 0.0 Actor Loss: 0.0426 VF Loss: 1.3777\n",
      "Episode: 240 Total reward: 179.6 Episode length: 0.0 Actor Loss: 0.1005 VF Loss: 1.8512\n",
      "Episode: 250 Total reward: 146.9 Episode length: 0.0 Actor Loss: 0.2261 VF Loss: 2.5828\n",
      "Episode: 260 Total reward: 167.7 Episode length: 0.0 Actor Loss: 0.0821 VF Loss: 0.6238\n",
      "Episode: 270 Total reward: 161.7 Episode length: 0.0 Actor Loss: -0.0210 VF Loss: 1.1238\n",
      "Episode: 280 Total reward: 179.5 Episode length: 0.0 Actor Loss: -0.0241 VF Loss: 1.6442\n",
      "Episode: 290 Total reward: 148.8 Episode length: 0.0 Actor Loss: 0.0452 VF Loss: 0.5605\n",
      "Episode: 300 Total reward: 156.1 Episode length: 0.0 Actor Loss: -0.0005 VF Loss: 0.4477\n",
      "Episode: 310 Total reward: 174.6 Episode length: 0.0 Actor Loss: -0.0807 VF Loss: 1.2840\n",
      "Episode: 320 Total reward: 156.1 Episode length: 0.0 Actor Loss: -0.0017 VF Loss: 1.0530\n",
      "Episode: 330 Total reward: 184.6 Episode length: 0.0 Actor Loss: 0.0280 VF Loss: 0.5290\n",
      "Episode: 340 Total reward: 188.0 Episode length: 0.0 Actor Loss: -0.0634 VF Loss: 0.7191\n",
      "Episode: 350 Total reward: 168.7 Episode length: 0.0 Actor Loss: 0.1142 VF Loss: 0.6611\n",
      "Episode: 360 Total reward: 190.3 Episode length: 0.0 Actor Loss: -0.0265 VF Loss: 0.7036\n",
      "Episode: 370 Total reward: 194.2 Episode length: 0.0 Actor Loss: -0.0706 VF Loss: 1.2384\n",
      "Episode: 380 Total reward: 172.4 Episode length: 0.0 Actor Loss: -0.0218 VF Loss: 1.3361\n",
      "Episode: 390 Total reward: 188.6 Episode length: 0.0 Actor Loss: 0.2153 VF Loss: 2.3453\n",
      "Episode: 400 Total reward: 190.9 Episode length: 0.0 Actor Loss: 0.0228 VF Loss: 0.4939\n",
      "Episode: 410 Total reward: 193.3 Episode length: 0.0 Actor Loss: 0.0175 VF Loss: 0.5334\n",
      "Episode: 420 Total reward: 193.8 Episode length: 0.0 Actor Loss: 0.0225 VF Loss: 0.8099\n",
      "Episode: 430 Total reward: 180.5 Episode length: 0.0 Actor Loss: 0.0846 VF Loss: 0.5768\n",
      "Episode: 440 Total reward: 176.2 Episode length: 0.0 Actor Loss: -0.0183 VF Loss: 0.2503\n",
      "Episode: 450 Total reward: 183.6 Episode length: 0.0 Actor Loss: -0.0386 VF Loss: 0.7307\n",
      "Episode: 460 Total reward: 182.7 Episode length: 0.0 Actor Loss: 0.1157 VF Loss: 0.7127\n",
      "Episode: 470 Total reward: 192.5 Episode length: 0.0 Actor Loss: -0.0221 VF Loss: 1.0574\n",
      "Episode: 480 Total reward: 200.0 Episode length: 0.0 Actor Loss: -0.0911 VF Loss: 1.7193\n",
      "Episode: 490 Total reward: 200.0 Episode length: 0.0 Actor Loss: -0.0180 VF Loss: 1.3773\n",
      "Episode: 500 Total reward: 182.3 Episode length: 0.0 Actor Loss: 0.0873 VF Loss: 0.7235\n",
      "Episode: 510 Total reward: 195.0 Episode length: 0.0 Actor Loss: 0.0192 VF Loss: 0.6027\n",
      "Episode: 520 Total reward: 194.3 Episode length: 0.0 Actor Loss: -0.0178 VF Loss: 0.4131\n",
      "Episode: 530 Total reward: 195.4 Episode length: 0.0 Actor Loss: -0.0460 VF Loss: 1.5982\n",
      "Episode: 540 Total reward: 183.8 Episode length: 0.0 Actor Loss: 0.1182 VF Loss: 0.8661\n",
      "Episode: 550 Total reward: 185.3 Episode length: 0.0 Actor Loss: 0.0067 VF Loss: 0.4247\n",
      "Episode: 560 Total reward: 173.8 Episode length: 0.0 Actor Loss: 0.0582 VF Loss: 0.5212\n",
      "Episode: 570 Total reward: 184.5 Episode length: 0.0 Actor Loss: -0.0512 VF Loss: 1.4950\n",
      "Episode: 580 Total reward: 186.5 Episode length: 0.0 Actor Loss: 0.0826 VF Loss: 0.8145\n",
      "Episode: 590 Total reward: 189.2 Episode length: 0.0 Actor Loss: -0.0504 VF Loss: 1.4500\n",
      "Episode: 600 Total reward: 200.0 Episode length: 0.0 Actor Loss: -0.0378 VF Loss: 1.3819\n",
      "Episode: 610 Total reward: 190.3 Episode length: 0.0 Actor Loss: -0.0296 VF Loss: 0.6604\n",
      "Episode: 620 Total reward: 192.5 Episode length: 0.0 Actor Loss: -0.0266 VF Loss: 1.1568\n",
      "Episode: 630 Total reward: 198.4 Episode length: 0.0 Actor Loss: -0.0039 VF Loss: 0.9282\n",
      "Episode: 640 Total reward: 182.6 Episode length: 0.0 Actor Loss: -0.0797 VF Loss: 1.4555\n",
      "Episode: 650 Total reward: 190.8 Episode length: 0.0 Actor Loss: 0.0058 VF Loss: 1.4570\n",
      "Episode: 660 Total reward: 195.0 Episode length: 0.0 Actor Loss: -0.0800 VF Loss: 1.7363\n",
      "Episode: 670 Total reward: 200.0 Episode length: 0.0 Actor Loss: -0.0446 VF Loss: 1.4999\n",
      "Episode: 680 Total reward: 195.1 Episode length: 0.0 Actor Loss: -0.0346 VF Loss: 1.4334\n",
      "Episode: 690 Total reward: 200.0 Episode length: 0.0 Actor Loss: 0.0107 VF Loss: 1.0080\n",
      "Episode: 700 Total reward: 198.9 Episode length: 0.0 Actor Loss: -0.0063 VF Loss: 0.8543\n",
      "Episode: 710 Total reward: 196.4 Episode length: 0.0 Actor Loss: 0.0961 VF Loss: 0.9908\n",
      "Episode: 720 Total reward: 197.7 Episode length: 0.0 Actor Loss: 0.0383 VF Loss: 0.5078\n",
      "Episode: 730 Total reward: 189.9 Episode length: 0.0 Actor Loss: -0.0596 VF Loss: 1.3381\n",
      "Episode: 740 Total reward: 198.0 Episode length: 0.0 Actor Loss: 0.1192 VF Loss: 1.1897\n",
      "Episode: 750 Total reward: 195.4 Episode length: 0.0 Actor Loss: 0.1052 VF Loss: 0.6744\n",
      "Episode: 760 Total reward: 191.2 Episode length: 0.0 Actor Loss: 0.1397 VF Loss: 1.0730\n",
      "Episode: 770 Total reward: 195.1 Episode length: 0.0 Actor Loss: -0.0536 VF Loss: 1.5980\n",
      "Episode: 780 Total reward: 189.2 Episode length: 0.0 Actor Loss: -0.0774 VF Loss: 1.3673\n",
      "Episode: 790 Total reward: 185.5 Episode length: 0.0 Actor Loss: -0.0873 VF Loss: 1.6501\n",
      "Episode: 800 Total reward: 198.3 Episode length: 0.0 Actor Loss: 0.0570 VF Loss: 0.4839\n",
      "Episode: 810 Total reward: 200.0 Episode length: 0.0 Actor Loss: -0.0376 VF Loss: 1.4381\n",
      "Episode: 820 Total reward: 196.9 Episode length: 0.0 Actor Loss: 0.0011 VF Loss: 0.8184\n",
      "Episode: 830 Total reward: 195.4 Episode length: 0.0 Actor Loss: 0.0881 VF Loss: 0.9207\n",
      "Episode: 840 Total reward: 200.0 Episode length: 0.0 Actor Loss: -0.0100 VF Loss: 0.7242\n",
      "Episode: 850 Total reward: 192.4 Episode length: 0.0 Actor Loss: -0.0785 VF Loss: 1.7561\n",
      "Episode: 860 Total reward: 193.3 Episode length: 0.0 Actor Loss: -0.0179 VF Loss: 0.4417\n",
      "Episode: 870 Total reward: 193.6 Episode length: 0.0 Actor Loss: -0.0006 VF Loss: 1.5778\n",
      "Episode: 880 Total reward: 182.2 Episode length: 0.0 Actor Loss: -0.0326 VF Loss: 0.3074\n",
      "Episode: 890 Total reward: 188.7 Episode length: 0.0 Actor Loss: 0.0968 VF Loss: 0.8667\n",
      "Episode: 900 Total reward: 191.9 Episode length: 0.0 Actor Loss: -0.0568 VF Loss: 1.1749\n",
      "Episode: 910 Total reward: 200.0 Episode length: 0.0 Actor Loss: 0.0256 VF Loss: 1.4402\n",
      "Episode: 920 Total reward: 198.5 Episode length: 0.0 Actor Loss: 0.0035 VF Loss: 1.7331\n",
      "Episode: 930 Total reward: 198.0 Episode length: 0.0 Actor Loss: -0.0516 VF Loss: 1.5783\n",
      "Episode: 940 Total reward: 193.0 Episode length: 0.0 Actor Loss: 0.1665 VF Loss: 1.2837\n",
      "Episode: 950 Total reward: 187.1 Episode length: 0.0 Actor Loss: 0.0338 VF Loss: 0.2812\n",
      "Episode: 960 Total reward: 182.5 Episode length: 0.0 Actor Loss: 0.0297 VF Loss: 0.1732\n",
      "Episode: 970 Total reward: 180.3 Episode length: 0.0 Actor Loss: 0.0044 VF Loss: 0.2641\n",
      "Episode: 980 Total reward: 198.6 Episode length: 0.0 Actor Loss: 0.0142 VF Loss: 0.2357\n",
      "Episode: 990 Total reward: 195.4 Episode length: 0.0 Actor Loss: 0.1082 VF Loss: 0.7487\n"
     ]
    }
   ],
   "source": [
    "# key functions of Gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Initialize RL agent\n",
    "RL_cart = PolicyGradient(4,2,1e-3,1e-3,0.95)\n",
    "\n",
    "rewards = []\n",
    "stats_rewards_list = []\n",
    "# Roll out 1000 episodes\n",
    "for episode in range(1000):    \n",
    "    # Initiate one episode\n",
    "    observation, info = env.reset()\n",
    "    episode_length = 0\n",
    "    stats_actor_loss, stats_vf_loss = 0., 0.\n",
    "    \n",
    "    state_list, action_list, reward_list = [], [], []\n",
    "\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Roll out one episode\n",
    "    while (not terminated) and (not truncated):\n",
    "        #action = env.action_space.sample() # Use your policy here\n",
    "        #observation = torch.from_numpy(observation).to(device)\n",
    "        #p = RL_cart(observation)\n",
    "        action = RL_cart.select_action(observation)\n",
    "        next_observation, reward, terminated, truncated, _  = env.step(int(action))\n",
    "\n",
    "        # store agent's trajectory\n",
    "        state_list.append(observation)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "        observation = next_observation\n",
    "    \n",
    "    ep_rs_sum = sum(reward_list)\n",
    "    actor_loss, vf_loss = RL_cart.train(state_list, action_list, reward_list)\n",
    "    stats_rewards_list.append((episode, ep_rs_sum, episode_length))\n",
    "    stats_actor_loss += actor_loss\n",
    "    stats_vf_loss += vf_loss\n",
    "    total_reward = 0\n",
    "    episode_length = 0  \n",
    "    if episode % 10 == 0:\n",
    "        print('Episode: {}'.format(episode),\n",
    "            'Total reward: {:.1f}'.format(np.mean(stats_rewards_list[-10:],axis=0)[1]),\n",
    "            'Episode length: {:.1f}'.format(np.mean(stats_rewards_list[-10:],axis=0)[2]),\n",
    "            'Actor Loss: {:.4f}'.format(stats_actor_loss/10), \n",
    "            'VF Loss: {:.4f}'.format(stats_vf_loss/10))\n",
    "        stats_actor_loss, stats_vf_loss = 0., 0.\n",
    "\n",
    "    if 'running_reward' not in globals():\n",
    "        running_reward = ep_rs_sum\n",
    "    else:\n",
    "        running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "    rewards.append(ep_rs_sum)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2684a61d-db91-4793-be6f-e438c67ec8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -21.0 Episode length: 0.0 Actor Loss: -0.1362 VF Loss: 0.4062\n",
      "Episode: 10 Total reward: -20.1 Episode length: 0.0 Actor Loss: -0.0131 VF Loss: 0.0469\n",
      "Episode: 20 Total reward: -20.2 Episode length: 0.0 Actor Loss: 0.0017 VF Loss: 0.0621\n",
      "Episode: 30 Total reward: -20.6 Episode length: 0.0 Actor Loss: 0.0035 VF Loss: 0.0407\n",
      "Episode: 40 Total reward: -20.6 Episode length: 0.0 Actor Loss: -0.0244 VF Loss: 0.0443\n",
      "Episode: 50 Total reward: -19.8 Episode length: 0.0 Actor Loss: -0.0075 VF Loss: 0.0354\n",
      "Episode: 60 Total reward: -20.0 Episode length: 0.0 Actor Loss: 0.0122 VF Loss: 0.0307\n",
      "Episode: 70 Total reward: -19.3 Episode length: 0.0 Actor Loss: -0.0038 VF Loss: 0.0303\n",
      "Episode: 80 Total reward: -20.3 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0247\n",
      "Episode: 90 Total reward: -19.8 Episode length: 0.0 Actor Loss: 0.0040 VF Loss: 0.0429\n",
      "Episode: 100 Total reward: -19.9 Episode length: 0.0 Actor Loss: -0.0022 VF Loss: 0.0191\n",
      "Episode: 110 Total reward: -19.9 Episode length: 0.0 Actor Loss: -0.0042 VF Loss: 0.0142\n",
      "Episode: 120 Total reward: -19.0 Episode length: 0.0 Actor Loss: -0.0052 VF Loss: 0.0142\n",
      "Episode: 130 Total reward: -18.8 Episode length: 0.0 Actor Loss: 0.0013 VF Loss: 0.0185\n",
      "Episode: 140 Total reward: -19.4 Episode length: 0.0 Actor Loss: 0.0066 VF Loss: 0.0197\n",
      "Episode: 150 Total reward: -19.0 Episode length: 0.0 Actor Loss: -0.0030 VF Loss: 0.0176\n",
      "Episode: 160 Total reward: -18.9 Episode length: 0.0 Actor Loss: 0.0025 VF Loss: 0.0148\n",
      "Episode: 170 Total reward: -17.8 Episode length: 0.0 Actor Loss: 0.0007 VF Loss: 0.0187\n",
      "Episode: 180 Total reward: -17.9 Episode length: 0.0 Actor Loss: -0.0061 VF Loss: 0.0266\n",
      "Episode: 190 Total reward: -18.6 Episode length: 0.0 Actor Loss: 0.0012 VF Loss: 0.0196\n",
      "Episode: 200 Total reward: -19.0 Episode length: 0.0 Actor Loss: 0.0033 VF Loss: 0.0182\n",
      "Episode: 210 Total reward: -19.1 Episode length: 0.0 Actor Loss: -0.0049 VF Loss: 0.0125\n",
      "Episode: 220 Total reward: -17.7 Episode length: 0.0 Actor Loss: -0.0048 VF Loss: 0.0114\n",
      "Episode: 230 Total reward: -18.0 Episode length: 0.0 Actor Loss: -0.0007 VF Loss: 0.0102\n",
      "Episode: 240 Total reward: -16.5 Episode length: 0.0 Actor Loss: 0.0076 VF Loss: 0.0276\n",
      "Episode: 250 Total reward: -17.6 Episode length: 0.0 Actor Loss: -0.0010 VF Loss: 0.0155\n",
      "Episode: 260 Total reward: -18.4 Episode length: 0.0 Actor Loss: 0.0008 VF Loss: 0.0125\n",
      "Episode: 270 Total reward: -18.0 Episode length: 0.0 Actor Loss: -0.0028 VF Loss: 0.0182\n",
      "Episode: 280 Total reward: -16.4 Episode length: 0.0 Actor Loss: 0.0012 VF Loss: 0.0157\n",
      "Episode: 290 Total reward: -16.6 Episode length: 0.0 Actor Loss: -0.0018 VF Loss: 0.0120\n",
      "Episode: 300 Total reward: -18.4 Episode length: 0.0 Actor Loss: 0.0025 VF Loss: 0.0104\n",
      "Episode: 310 Total reward: -16.1 Episode length: 0.0 Actor Loss: -0.0021 VF Loss: 0.0140\n",
      "Episode: 320 Total reward: -16.5 Episode length: 0.0 Actor Loss: -0.0018 VF Loss: 0.0139\n",
      "Episode: 330 Total reward: -17.2 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0263\n",
      "Episode: 340 Total reward: -18.4 Episode length: 0.0 Actor Loss: 0.0009 VF Loss: 0.0127\n",
      "Episode: 350 Total reward: -17.2 Episode length: 0.0 Actor Loss: -0.0014 VF Loss: 0.0120\n",
      "Episode: 360 Total reward: -16.2 Episode length: 0.0 Actor Loss: 0.0021 VF Loss: 0.0238\n",
      "Episode: 370 Total reward: -17.3 Episode length: 0.0 Actor Loss: -0.0017 VF Loss: 0.0113\n",
      "Episode: 380 Total reward: -15.1 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0136\n",
      "Episode: 390 Total reward: -15.8 Episode length: 0.0 Actor Loss: -0.0007 VF Loss: 0.0107\n",
      "Episode: 400 Total reward: -14.5 Episode length: 0.0 Actor Loss: 0.0011 VF Loss: 0.0161\n",
      "Episode: 410 Total reward: -14.6 Episode length: 0.0 Actor Loss: -0.0005 VF Loss: 0.0139\n",
      "Episode: 420 Total reward: -15.3 Episode length: 0.0 Actor Loss: 0.0019 VF Loss: 0.0212\n",
      "Episode: 430 Total reward: -17.5 Episode length: 0.0 Actor Loss: 0.0018 VF Loss: 0.0166\n",
      "Episode: 440 Total reward: -14.2 Episode length: 0.0 Actor Loss: 0.0010 VF Loss: 0.0140\n",
      "Episode: 450 Total reward: -15.3 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0112\n",
      "Episode: 460 Total reward: -16.0 Episode length: 0.0 Actor Loss: -0.0008 VF Loss: 0.0145\n",
      "Episode: 470 Total reward: -16.5 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0130\n",
      "Episode: 480 Total reward: -16.7 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0092\n",
      "Episode: 490 Total reward: -15.0 Episode length: 0.0 Actor Loss: -0.0021 VF Loss: 0.0147\n",
      "Episode: 500 Total reward: -16.0 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0140\n",
      "Episode: 510 Total reward: -16.8 Episode length: 0.0 Actor Loss: 0.0018 VF Loss: 0.0141\n",
      "Episode: 520 Total reward: -15.3 Episode length: 0.0 Actor Loss: 0.0009 VF Loss: 0.0137\n",
      "Episode: 530 Total reward: -15.8 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0101\n",
      "Episode: 540 Total reward: -16.2 Episode length: 0.0 Actor Loss: 0.0012 VF Loss: 0.0158\n",
      "Episode: 550 Total reward: -16.0 Episode length: 0.0 Actor Loss: -0.0030 VF Loss: 0.0175\n",
      "Episode: 560 Total reward: -15.2 Episode length: 0.0 Actor Loss: 0.0009 VF Loss: 0.0122\n",
      "Episode: 570 Total reward: -15.1 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0119\n",
      "Episode: 580 Total reward: -15.8 Episode length: 0.0 Actor Loss: 0.0020 VF Loss: 0.0184\n",
      "Episode: 590 Total reward: -13.6 Episode length: 0.0 Actor Loss: 0.0013 VF Loss: 0.0136\n",
      "Episode: 600 Total reward: -15.8 Episode length: 0.0 Actor Loss: 0.0011 VF Loss: 0.0116\n",
      "Episode: 610 Total reward: -14.9 Episode length: 0.0 Actor Loss: -0.0010 VF Loss: 0.0140\n",
      "Episode: 620 Total reward: -13.5 Episode length: 0.0 Actor Loss: 0.0011 VF Loss: 0.0115\n",
      "Episode: 630 Total reward: -15.0 Episode length: 0.0 Actor Loss: 0.0010 VF Loss: 0.0178\n",
      "Episode: 640 Total reward: -14.4 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0158\n",
      "Episode: 650 Total reward: -15.3 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0107\n",
      "Episode: 660 Total reward: -13.3 Episode length: 0.0 Actor Loss: 0.0006 VF Loss: 0.0122\n",
      "Episode: 670 Total reward: -12.4 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0106\n",
      "Episode: 680 Total reward: -14.6 Episode length: 0.0 Actor Loss: -0.0020 VF Loss: 0.0142\n",
      "Episode: 690 Total reward: -14.8 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0108\n",
      "Episode: 700 Total reward: -14.4 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0142\n",
      "Episode: 710 Total reward: -15.1 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0092\n",
      "Episode: 720 Total reward: -13.8 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0119\n",
      "Episode: 730 Total reward: -13.9 Episode length: 0.0 Actor Loss: 0.0008 VF Loss: 0.0079\n",
      "Episode: 740 Total reward: -15.0 Episode length: 0.0 Actor Loss: -0.0015 VF Loss: 0.0154\n",
      "Episode: 750 Total reward: -13.3 Episode length: 0.0 Actor Loss: -0.0005 VF Loss: 0.0117\n",
      "Episode: 760 Total reward: -14.5 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0210\n",
      "Episode: 770 Total reward: -13.1 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0129\n",
      "Episode: 780 Total reward: -14.7 Episode length: 0.0 Actor Loss: 0.0012 VF Loss: 0.0089\n",
      "Episode: 790 Total reward: -15.0 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0100\n",
      "Episode: 800 Total reward: -14.6 Episode length: 0.0 Actor Loss: -0.0007 VF Loss: 0.0109\n",
      "Episode: 810 Total reward: -14.7 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0133\n",
      "Episode: 820 Total reward: -15.0 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0104\n",
      "Episode: 830 Total reward: -13.8 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0136\n",
      "Episode: 840 Total reward: -14.2 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0109\n",
      "Episode: 850 Total reward: -14.1 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0121\n",
      "Episode: 860 Total reward: -13.5 Episode length: 0.0 Actor Loss: 0.0006 VF Loss: 0.0104\n",
      "Episode: 870 Total reward: -14.9 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0123\n",
      "Episode: 880 Total reward: -14.4 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0105\n",
      "Episode: 890 Total reward: -13.1 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0109\n",
      "Episode: 900 Total reward: -11.6 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0167\n",
      "Episode: 910 Total reward: -15.0 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0130\n",
      "Episode: 920 Total reward: -14.1 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0096\n",
      "Episode: 930 Total reward: -13.1 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0107\n",
      "Episode: 940 Total reward: -14.1 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0104\n",
      "Episode: 950 Total reward: -12.1 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0153\n",
      "Episode: 960 Total reward: -11.4 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0130\n",
      "Episode: 970 Total reward: -11.8 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0131\n",
      "Episode: 980 Total reward: -14.2 Episode length: 0.0 Actor Loss: -0.0007 VF Loss: 0.0142\n",
      "Episode: 990 Total reward: -10.9 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0127\n",
      "Episode: 1000 Total reward: -10.0 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0123\n",
      "Episode: 1010 Total reward: -10.3 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0134\n",
      "Episode: 1020 Total reward: -13.2 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0104\n",
      "Episode: 1030 Total reward: -13.2 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0140\n",
      "Episode: 1040 Total reward: -12.8 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0114\n",
      "Episode: 1050 Total reward: -12.3 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0135\n",
      "Episode: 1060 Total reward: -14.4 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0089\n",
      "Episode: 1070 Total reward: -10.9 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0160\n",
      "Episode: 1080 Total reward: -13.3 Episode length: 0.0 Actor Loss: -0.0010 VF Loss: 0.0140\n",
      "Episode: 1090 Total reward: -10.9 Episode length: 0.0 Actor Loss: -0.0005 VF Loss: 0.0137\n",
      "Episode: 1100 Total reward: -10.4 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0096\n",
      "Episode: 1110 Total reward: -11.2 Episode length: 0.0 Actor Loss: 0.0007 VF Loss: 0.0101\n",
      "Episode: 1120 Total reward: -11.5 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0104\n",
      "Episode: 1130 Total reward: -11.3 Episode length: 0.0 Actor Loss: 0.0007 VF Loss: 0.0112\n",
      "Episode: 1140 Total reward: -12.5 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0122\n",
      "Episode: 1150 Total reward: -12.8 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0087\n",
      "Episode: 1160 Total reward: -12.7 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0107\n",
      "Episode: 1170 Total reward: -11.9 Episode length: 0.0 Actor Loss: 0.0008 VF Loss: 0.0147\n",
      "Episode: 1180 Total reward: -12.2 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0109\n",
      "Episode: 1190 Total reward: -13.4 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0078\n",
      "Episode: 1200 Total reward: -14.1 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0085\n",
      "Episode: 1210 Total reward: -13.7 Episode length: 0.0 Actor Loss: -0.0006 VF Loss: 0.0094\n",
      "Episode: 1220 Total reward: -14.6 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0074\n",
      "Episode: 1230 Total reward: -14.1 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0109\n",
      "Episode: 1240 Total reward: -12.6 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0083\n",
      "Episode: 1250 Total reward: -11.6 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0100\n",
      "Episode: 1260 Total reward: -11.9 Episode length: 0.0 Actor Loss: -0.0005 VF Loss: 0.0117\n",
      "Episode: 1270 Total reward: -12.2 Episode length: 0.0 Actor Loss: 0.0008 VF Loss: 0.0129\n",
      "Episode: 1280 Total reward: -10.1 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0109\n",
      "Episode: 1290 Total reward: -9.1 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0117\n",
      "Episode: 1300 Total reward: -12.2 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0106\n",
      "Episode: 1310 Total reward: -11.0 Episode length: 0.0 Actor Loss: -0.0005 VF Loss: 0.0100\n",
      "Episode: 1320 Total reward: -13.1 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0090\n",
      "Episode: 1330 Total reward: -12.9 Episode length: 0.0 Actor Loss: -0.0009 VF Loss: 0.0074\n",
      "Episode: 1340 Total reward: -12.8 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0133\n",
      "Episode: 1350 Total reward: -11.1 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0102\n",
      "Episode: 1360 Total reward: -13.2 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0148\n",
      "Episode: 1370 Total reward: -13.0 Episode length: 0.0 Actor Loss: -0.0007 VF Loss: 0.0119\n",
      "Episode: 1380 Total reward: -10.1 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0113\n",
      "Episode: 1390 Total reward: -11.8 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0102\n",
      "Episode: 1400 Total reward: -11.7 Episode length: 0.0 Actor Loss: -0.0004 VF Loss: 0.0134\n",
      "Episode: 1410 Total reward: -11.1 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0130\n",
      "Episode: 1420 Total reward: -9.9 Episode length: 0.0 Actor Loss: -0.0009 VF Loss: 0.0124\n",
      "Episode: 1430 Total reward: -10.9 Episode length: 0.0 Actor Loss: -0.0005 VF Loss: 0.0094\n",
      "Episode: 1440 Total reward: -12.1 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0128\n",
      "Episode: 1450 Total reward: -13.2 Episode length: 0.0 Actor Loss: -0.0004 VF Loss: 0.0116\n",
      "Episode: 1460 Total reward: -12.1 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0124\n",
      "Episode: 1470 Total reward: -10.6 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0082\n",
      "Episode: 1480 Total reward: -10.4 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0099\n",
      "Episode: 1490 Total reward: -11.9 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0091\n",
      "Episode: 1500 Total reward: -12.1 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0101\n",
      "Episode: 1510 Total reward: -12.4 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0108\n",
      "Episode: 1520 Total reward: -11.4 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0097\n",
      "Episode: 1530 Total reward: -11.7 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0143\n",
      "Episode: 1540 Total reward: -11.4 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0094\n",
      "Episode: 1550 Total reward: -11.3 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0124\n",
      "Episode: 1560 Total reward: -11.5 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0092\n",
      "Episode: 1570 Total reward: -11.3 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0098\n",
      "Episode: 1580 Total reward: -10.8 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0087\n",
      "Episode: 1590 Total reward: -12.8 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0112\n",
      "Episode: 1600 Total reward: -12.2 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0113\n",
      "Episode: 1610 Total reward: -12.5 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0093\n",
      "Episode: 1620 Total reward: -10.3 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0097\n",
      "Episode: 1630 Total reward: -11.1 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0091\n",
      "Episode: 1640 Total reward: -12.6 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0140\n",
      "Episode: 1650 Total reward: -11.3 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0106\n",
      "Episode: 1660 Total reward: -9.8 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0105\n",
      "Episode: 1670 Total reward: -12.6 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0121\n",
      "Episode: 1680 Total reward: -12.6 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0106\n",
      "Episode: 1690 Total reward: -11.6 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0091\n",
      "Episode: 1700 Total reward: -12.9 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0101\n",
      "Episode: 1710 Total reward: -10.7 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0100\n",
      "Episode: 1720 Total reward: -10.4 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0105\n",
      "Episode: 1730 Total reward: -10.7 Episode length: 0.0 Actor Loss: 0.0005 VF Loss: 0.0139\n",
      "Episode: 1740 Total reward: -10.1 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0150\n",
      "Episode: 1750 Total reward: -11.1 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0087\n",
      "Episode: 1760 Total reward: -10.5 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0120\n",
      "Episode: 1770 Total reward: -10.7 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0082\n",
      "Episode: 1780 Total reward: -13.7 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0094\n",
      "Episode: 1790 Total reward: -11.7 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0104\n",
      "Episode: 1800 Total reward: -11.5 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0095\n",
      "Episode: 1810 Total reward: -7.9 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0093\n",
      "Episode: 1820 Total reward: -7.3 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0107\n",
      "Episode: 1830 Total reward: -9.7 Episode length: 0.0 Actor Loss: -0.0001 VF Loss: 0.0088\n",
      "Episode: 1840 Total reward: -9.8 Episode length: 0.0 Actor Loss: -0.0008 VF Loss: 0.0086\n",
      "Episode: 1850 Total reward: -11.5 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0084\n",
      "Episode: 1860 Total reward: -10.3 Episode length: 0.0 Actor Loss: 0.0007 VF Loss: 0.0097\n",
      "Episode: 1870 Total reward: -11.8 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0106\n",
      "Episode: 1880 Total reward: -9.7 Episode length: 0.0 Actor Loss: -0.0004 VF Loss: 0.0089\n",
      "Episode: 1890 Total reward: -8.9 Episode length: 0.0 Actor Loss: 0.0003 VF Loss: 0.0111\n",
      "Episode: 1900 Total reward: -10.1 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0110\n",
      "Episode: 1910 Total reward: -9.8 Episode length: 0.0 Actor Loss: -0.0003 VF Loss: 0.0090\n",
      "Episode: 1920 Total reward: -8.6 Episode length: 0.0 Actor Loss: 0.0004 VF Loss: 0.0111\n",
      "Episode: 1930 Total reward: -13.5 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0089\n",
      "Episode: 1940 Total reward: -11.7 Episode length: 0.0 Actor Loss: 0.0000 VF Loss: 0.0115\n",
      "Episode: 1950 Total reward: -9.6 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0114\n",
      "Episode: 1960 Total reward: -8.9 Episode length: 0.0 Actor Loss: -0.0002 VF Loss: 0.0113\n",
      "Episode: 1970 Total reward: -11.1 Episode length: 0.0 Actor Loss: 0.0001 VF Loss: 0.0102\n",
      "Episode: 1980 Total reward: -11.0 Episode length: 0.0 Actor Loss: 0.0002 VF Loss: 0.0102\n",
      "Episode: 1990 Total reward: -9.8 Episode length: 0.0 Actor Loss: -0.0000 VF Loss: 0.0099\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "RL_pong = PolicyGradient(6400,2,1e-3,1e-3,0.99)\n",
    "rewards = []\n",
    "stats_rewards_list = []\n",
    "for episode in range(2000):\n",
    "    # Initiate one episode\n",
    "    observation, info = env.reset()\n",
    "    episode_length = 0\n",
    "    stats_actor_loss, stats_vf_loss = 0., 0.\n",
    "    \n",
    "    state_list, action_list, reward_list = [], [], []\n",
    "\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Roll out one episode\n",
    "    while (not terminated) and (not truncated):\n",
    "        #action = env.action_space.sample() # Use your policy here\n",
    "        observation = preprocess(observation)\n",
    "        action = RL_pong.select_action(observation)\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action+2)\n",
    "\n",
    "        # store agent's trajectory\n",
    "        state_list.append(observation)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "        observation = next_observation\n",
    "\n",
    "    ep_rs_sum = sum(reward_list)\n",
    "\n",
    "    if 'running_reward' not in globals():\n",
    "        running_reward = ep_rs_sum\n",
    "    else:\n",
    "        running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "        \n",
    "    actor_loss, vf_loss = RL_pong.train(state_list, action_list, reward_list)\n",
    "    stats_rewards_list.append((episode, ep_rs_sum, episode_length))\n",
    "    stats_actor_loss += actor_loss\n",
    "    stats_vf_loss += vf_loss\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "    \n",
    "    rewards.append(ep_rs_sum)\n",
    "    if episode % 10 == 0:\n",
    "        print('Episode: {}'.format(episode),\n",
    "            'Total reward: {:.1f}'.format(np.mean(stats_rewards_list[-10:],axis=0)[1]),\n",
    "            'Episode length: {:.1f}'.format(np.mean(stats_rewards_list[-10:],axis=0)[2]),\n",
    "            'Actor Loss: {:.4f}'.format(stats_actor_loss/10), \n",
    "            'VF Loss: {:.4f}'.format(stats_vf_loss/10))\n",
    "        stats_actor_loss, stats_vf_loss = 0., 0.\n",
    "    if episode % 100 == 0:\n",
    "        np.save('./rewards_base.npy', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319fe8c-2fa8-45ad-aad9-39154c82c594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
