{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e484f6f-a202-4834-b732-813e5554e7f2",
   "metadata": {},
   "source": [
    "## Import required dependencies and check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a412cd0-7be6-4524-bf22-17511051a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Oct 17 14:48:31 2023\n",
    "\n",
    "@author: dinglin\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8994a1-13f0-493a-9942-43570609745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 19 06:12:15 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 22%   39C    P2    62W / 250W |  10489MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 22%   23C    P8    21W / 250W |    341MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 22%   22C    P8     5W / 250W |    341MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 22%   21C    P8     4W / 250W |    341MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  On   | 00000000:88:00.0 Off |                  N/A |\n",
      "| 22%   35C    P2    49W / 250W |  10161MiB / 11264MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  On   | 00000000:89:00.0 Off |                  N/A |\n",
      "| 22%   40C    P2    51W / 250W |  10239MiB / 11264MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce ...  On   | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 22%   22C    P8     6W / 250W |    341MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce ...  On   | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 22%   22C    P8    19W / 250W |    341MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1313008      C   python                          10486MiB |\n",
      "|    1   N/A  N/A   1313008      C   python                            338MiB |\n",
      "|    2   N/A  N/A   1313008      C   python                            338MiB |\n",
      "|    3   N/A  N/A   1313008      C   python                            338MiB |\n",
      "|    4   N/A  N/A   1019816      C   ...s/nachuan_msia/bin/python     9820MiB |\n",
      "|    4   N/A  N/A   1313008      C   python                            338MiB |\n",
      "|    5   N/A  N/A    556860      C   ...da/envs/mldsrl/bin/python     9898MiB |\n",
      "|    5   N/A  N/A   1313008      C   python                            338MiB |\n",
      "|    6   N/A  N/A   1313008      C   python                            338MiB |\n",
      "|    7   N/A  N/A   1313008      C   python                            338MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69c1df5-da65-44b3-acb8-350ee85f735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194eef7a-9583-4621-97ed-7dc4f7ac2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess game frames\n",
    "def preprocess(frame):\n",
    "    frame = frame[35:195]\n",
    "    frame = frame[::2,::2,0]\n",
    "    frame[frame == 144] = 0\n",
    "    frame[frame == 109] = 0\n",
    "    frame[frame != 0] = 1\n",
    "    return np.reshape(frame.astype(np.float32), 6400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e33488-6ca0-4803-8b2c-976159eacc8a",
   "metadata": {},
   "source": [
    "## Define Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8735c2d7-d8c5-44e0-abd7-e0f7164812bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(Policy, self).__init__()\n",
    "        # memories\n",
    "        self.ep_obs, self.ep_as, self.ep_rs, self.log_p = [], [], [], []\n",
    "        # parameters\n",
    "        self.gamma = 0.99\n",
    "        # neural networks\n",
    "        self.layer1 = nn.Linear(n_inputs, 200)\n",
    "        self.layer2 = nn.Linear(200, n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def choose_action(self, p):\n",
    "        action = Categorical(p).sample()\n",
    "        self.log_p.append(torch.log(p[action]))\n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, s, a, r):\n",
    "        self.ep_obs.append(np.array([s], np.float32))\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "    \n",
    "    def discount_reward(self):\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "\n",
    "        for t in reversed(range(0, len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= (np.std(discounted_ep_rs)+1e-9)\n",
    "        return discounted_ep_rs\n",
    "        \n",
    "    def learn(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        discounted_ep_rs_norm = self.discount_reward()\n",
    "        loss = []\n",
    "        for q, l in zip(discounted_ep_rs_norm, self.log_p):\n",
    "            loss.append(-l * q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(loss).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.ep_obs, self.ep_as, self.ep_rs, self.log_p  = [], [], [], []\n",
    "\n",
    "def smoothing_plot(rewards, window = 99, save = \"True\"):\n",
    "    # Sample rewards data (replace this with your actual rewards)\n",
    "    episodes = list(range(1, len(rewards)+1))\n",
    "\n",
    "    # Apply a smoothing filter (Savitzky-Golay filter) to the rewards\n",
    "    smoothed_rewards = savgol_filter(rewards, window, 3)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, rewards, label='Original Rewards', color='lightgray', alpha=0.7)\n",
    "    plt.plot(episodes, smoothed_rewards, label='Smoothed Rewards', color='blue')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Rewards per Episode with Smoothing')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save:\n",
    "        plt.savefig(\"rewards.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b0913-c67b-45bb-93d0-4d504c1e624c",
   "metadata": {},
   "source": [
    "## Cart-pole Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc0e41c-a938-4244-8312-86e72832552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0, episode 0, rewards 15.0\n",
      "CartPole-v0, episode 10, rewards 30.0\n",
      "CartPole-v0, episode 20, rewards 17.0\n",
      "CartPole-v0, episode 30, rewards 44.0\n",
      "CartPole-v0, episode 40, rewards 20.0\n",
      "CartPole-v0, episode 50, rewards 58.0\n",
      "CartPole-v0, episode 60, rewards 19.0\n",
      "CartPole-v0, episode 70, rewards 43.0\n",
      "CartPole-v0, episode 80, rewards 74.0\n",
      "CartPole-v0, episode 90, rewards 48.0\n",
      "CartPole-v0, episode 100, rewards 21.0\n",
      "CartPole-v0, episode 110, rewards 33.0\n",
      "CartPole-v0, episode 120, rewards 47.0\n",
      "CartPole-v0, episode 130, rewards 101.0\n",
      "CartPole-v0, episode 140, rewards 20.0\n",
      "CartPole-v0, episode 150, rewards 20.0\n",
      "CartPole-v0, episode 160, rewards 78.0\n",
      "CartPole-v0, episode 170, rewards 33.0\n",
      "CartPole-v0, episode 180, rewards 19.0\n",
      "CartPole-v0, episode 190, rewards 25.0\n",
      "CartPole-v0, episode 200, rewards 99.0\n",
      "CartPole-v0, episode 210, rewards 200.0\n",
      "CartPole-v0, episode 220, rewards 107.0\n",
      "CartPole-v0, episode 230, rewards 37.0\n",
      "CartPole-v0, episode 240, rewards 96.0\n",
      "CartPole-v0, episode 250, rewards 136.0\n",
      "CartPole-v0, episode 260, rewards 41.0\n",
      "CartPole-v0, episode 270, rewards 86.0\n",
      "CartPole-v0, episode 280, rewards 54.0\n",
      "CartPole-v0, episode 290, rewards 121.0\n",
      "CartPole-v0, episode 300, rewards 187.0\n",
      "CartPole-v0, episode 310, rewards 166.0\n",
      "CartPole-v0, episode 320, rewards 171.0\n",
      "CartPole-v0, episode 330, rewards 118.0\n",
      "CartPole-v0, episode 340, rewards 200.0\n",
      "CartPole-v0, episode 350, rewards 161.0\n",
      "CartPole-v0, episode 360, rewards 200.0\n",
      "CartPole-v0, episode 370, rewards 200.0\n",
      "CartPole-v0, episode 380, rewards 200.0\n",
      "CartPole-v0, episode 390, rewards 123.0\n",
      "CartPole-v0, episode 400, rewards 101.0\n",
      "CartPole-v0, episode 410, rewards 200.0\n",
      "CartPole-v0, episode 420, rewards 200.0\n",
      "CartPole-v0, episode 430, rewards 200.0\n",
      "CartPole-v0, episode 440, rewards 200.0\n",
      "CartPole-v0, episode 450, rewards 200.0\n",
      "CartPole-v0, episode 460, rewards 200.0\n",
      "CartPole-v0, episode 470, rewards 200.0\n",
      "CartPole-v0, episode 480, rewards 172.0\n",
      "CartPole-v0, episode 490, rewards 200.0\n",
      "CartPole-v0, episode 500, rewards 149.0\n",
      "CartPole-v0, episode 510, rewards 200.0\n",
      "CartPole-v0, episode 520, rewards 200.0\n",
      "CartPole-v0, episode 530, rewards 197.0\n",
      "CartPole-v0, episode 540, rewards 183.0\n",
      "CartPole-v0, episode 550, rewards 112.0\n",
      "CartPole-v0, episode 560, rewards 123.0\n",
      "CartPole-v0, episode 570, rewards 200.0\n",
      "CartPole-v0, episode 580, rewards 50.0\n",
      "CartPole-v0, episode 590, rewards 200.0\n",
      "CartPole-v0, episode 600, rewards 45.0\n",
      "CartPole-v0, episode 610, rewards 200.0\n",
      "CartPole-v0, episode 620, rewards 157.0\n",
      "CartPole-v0, episode 630, rewards 200.0\n",
      "CartPole-v0, episode 640, rewards 111.0\n",
      "CartPole-v0, episode 650, rewards 200.0\n",
      "CartPole-v0, episode 660, rewards 200.0\n",
      "CartPole-v0, episode 670, rewards 86.0\n",
      "CartPole-v0, episode 680, rewards 127.0\n",
      "CartPole-v0, episode 690, rewards 196.0\n",
      "CartPole-v0, episode 700, rewards 157.0\n",
      "CartPole-v0, episode 710, rewards 100.0\n",
      "CartPole-v0, episode 720, rewards 200.0\n",
      "CartPole-v0, episode 730, rewards 158.0\n",
      "CartPole-v0, episode 740, rewards 60.0\n",
      "CartPole-v0, episode 750, rewards 200.0\n",
      "CartPole-v0, episode 760, rewards 138.0\n",
      "CartPole-v0, episode 770, rewards 128.0\n",
      "CartPole-v0, episode 780, rewards 135.0\n",
      "CartPole-v0, episode 790, rewards 200.0\n",
      "CartPole-v0, episode 800, rewards 200.0\n",
      "CartPole-v0, episode 810, rewards 168.0\n",
      "CartPole-v0, episode 820, rewards 75.0\n",
      "CartPole-v0, episode 830, rewards 200.0\n",
      "CartPole-v0, episode 840, rewards 200.0\n",
      "CartPole-v0, episode 850, rewards 157.0\n",
      "CartPole-v0, episode 860, rewards 200.0\n",
      "CartPole-v0, episode 870, rewards 200.0\n",
      "CartPole-v0, episode 880, rewards 200.0\n",
      "CartPole-v0, episode 890, rewards 200.0\n",
      "CartPole-v0, episode 900, rewards 168.0\n",
      "CartPole-v0, episode 910, rewards 128.0\n",
      "CartPole-v0, episode 920, rewards 161.0\n",
      "CartPole-v0, episode 930, rewards 173.0\n",
      "CartPole-v0, episode 940, rewards 105.0\n",
      "CartPole-v0, episode 950, rewards 103.0\n",
      "CartPole-v0, episode 960, rewards 162.0\n",
      "CartPole-v0, episode 970, rewards 132.0\n",
      "CartPole-v0, episode 980, rewards 120.0\n",
      "CartPole-v0, episode 990, rewards 123.0\n"
     ]
    }
   ],
   "source": [
    "# key functions of Gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Initialize RL agent\n",
    "RL_cart = Policy(4, 2).to(device)\n",
    "\n",
    "rewards = []\n",
    "# Roll out 1000 episodes\n",
    "for episode in range(1000):\n",
    "    \n",
    "    # Initiate one episode\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Roll out one episode\n",
    "    while (not terminated) and (not truncated):\n",
    "        #action = env.action_space.sample() # Use your policy here\n",
    "        observation = torch.from_numpy(observation).to(device)\n",
    "        p = RL_cart(observation)\n",
    "        action = RL_cart.choose_action(p)\n",
    "        observation, reward, terminated, truncated, info = env.step(int(action))\n",
    "\n",
    "        RL_cart.store_transition(observation, action, reward)\n",
    "    \n",
    "    ep_rs_sum = sum(RL_cart.ep_rs)\n",
    "\n",
    "    if 'running_reward' not in globals():\n",
    "        running_reward = ep_rs_sum\n",
    "    else:\n",
    "        running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "    rewards.append(ep_rs_sum)\n",
    "    if episode%10 == 0:\n",
    "        print(f\"CartPole-v0, episode {episode}, rewards {ep_rs_sum}\")\n",
    "    vt = RL_cart.learn()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4a862-892d-461c-9cc6-fe90834e843b",
   "metadata": {},
   "source": [
    "## Pong game Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b31a14-f482-4d71-b5da-b632c31c43a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pong-v0, episode 0, rewards -21.0\n",
      "Pong-v0, episode 10, rewards -20.0\n",
      "Pong-v0, episode 20, rewards -20.0\n",
      "Pong-v0, episode 30, rewards -20.0\n",
      "Pong-v0, episode 40, rewards -20.0\n",
      "Pong-v0, episode 50, rewards -19.0\n",
      "Pong-v0, episode 60, rewards -21.0\n",
      "Pong-v0, episode 70, rewards -19.0\n",
      "Pong-v0, episode 80, rewards -19.0\n",
      "Pong-v0, episode 90, rewards -20.0\n",
      "Pong-v0, episode 100, rewards -18.0\n",
      "Pong-v0, episode 110, rewards -20.0\n",
      "Pong-v0, episode 120, rewards -18.0\n",
      "Pong-v0, episode 130, rewards -17.0\n",
      "Pong-v0, episode 140, rewards -19.0\n",
      "Pong-v0, episode 150, rewards -18.0\n",
      "Pong-v0, episode 160, rewards -19.0\n",
      "Pong-v0, episode 170, rewards -18.0\n",
      "Pong-v0, episode 180, rewards -17.0\n",
      "Pong-v0, episode 190, rewards -15.0\n",
      "Pong-v0, episode 200, rewards -18.0\n",
      "Pong-v0, episode 210, rewards -18.0\n",
      "Pong-v0, episode 220, rewards -19.0\n",
      "Pong-v0, episode 230, rewards -10.0\n",
      "Pong-v0, episode 240, rewards -17.0\n",
      "Pong-v0, episode 250, rewards -19.0\n",
      "Pong-v0, episode 260, rewards -14.0\n",
      "Pong-v0, episode 270, rewards -13.0\n",
      "Pong-v0, episode 280, rewards -19.0\n",
      "Pong-v0, episode 290, rewards -17.0\n",
      "Pong-v0, episode 300, rewards -14.0\n",
      "Pong-v0, episode 310, rewards -12.0\n",
      "Pong-v0, episode 320, rewards -15.0\n",
      "Pong-v0, episode 330, rewards -17.0\n",
      "Pong-v0, episode 340, rewards -15.0\n",
      "Pong-v0, episode 350, rewards -15.0\n",
      "Pong-v0, episode 360, rewards -10.0\n",
      "Pong-v0, episode 370, rewards -13.0\n",
      "Pong-v0, episode 380, rewards -13.0\n",
      "Pong-v0, episode 390, rewards -13.0\n",
      "Pong-v0, episode 400, rewards -19.0\n",
      "Pong-v0, episode 410, rewards -13.0\n",
      "Pong-v0, episode 420, rewards -13.0\n",
      "Pong-v0, episode 430, rewards -17.0\n",
      "Pong-v0, episode 440, rewards -17.0\n",
      "Pong-v0, episode 450, rewards -14.0\n",
      "Pong-v0, episode 460, rewards -15.0\n",
      "Pong-v0, episode 470, rewards -19.0\n",
      "Pong-v0, episode 480, rewards -14.0\n",
      "Pong-v0, episode 490, rewards -15.0\n",
      "Pong-v0, episode 500, rewards -15.0\n",
      "Pong-v0, episode 510, rewards -15.0\n",
      "Pong-v0, episode 520, rewards -12.0\n",
      "Pong-v0, episode 530, rewards -17.0\n",
      "Pong-v0, episode 540, rewards -14.0\n",
      "Pong-v0, episode 550, rewards -15.0\n",
      "Pong-v0, episode 560, rewards -14.0\n",
      "Pong-v0, episode 570, rewards -15.0\n",
      "Pong-v0, episode 580, rewards -13.0\n",
      "Pong-v0, episode 590, rewards -6.0\n",
      "Pong-v0, episode 600, rewards -13.0\n",
      "Pong-v0, episode 610, rewards -9.0\n",
      "Pong-v0, episode 620, rewards -2.0\n",
      "Pong-v0, episode 630, rewards -11.0\n",
      "Pong-v0, episode 640, rewards -10.0\n",
      "Pong-v0, episode 650, rewards -8.0\n",
      "Pong-v0, episode 660, rewards -11.0\n",
      "Pong-v0, episode 670, rewards -5.0\n",
      "Pong-v0, episode 680, rewards -15.0\n",
      "Pong-v0, episode 690, rewards -13.0\n",
      "Pong-v0, episode 700, rewards -11.0\n",
      "Pong-v0, episode 710, rewards -13.0\n",
      "Pong-v0, episode 720, rewards -11.0\n",
      "Pong-v0, episode 730, rewards -6.0\n",
      "Pong-v0, episode 740, rewards -10.0\n",
      "Pong-v0, episode 750, rewards -11.0\n",
      "Pong-v0, episode 760, rewards -13.0\n",
      "Pong-v0, episode 770, rewards -14.0\n",
      "Pong-v0, episode 780, rewards -14.0\n",
      "Pong-v0, episode 790, rewards -12.0\n",
      "Pong-v0, episode 800, rewards -8.0\n",
      "Pong-v0, episode 810, rewards -14.0\n",
      "Pong-v0, episode 820, rewards -13.0\n",
      "Pong-v0, episode 830, rewards -10.0\n",
      "Pong-v0, episode 840, rewards -6.0\n",
      "Pong-v0, episode 850, rewards -11.0\n",
      "Pong-v0, episode 860, rewards -11.0\n",
      "Pong-v0, episode 870, rewards -17.0\n",
      "Pong-v0, episode 880, rewards -15.0\n",
      "Pong-v0, episode 890, rewards -13.0\n",
      "Pong-v0, episode 900, rewards -13.0\n",
      "Pong-v0, episode 910, rewards -8.0\n",
      "Pong-v0, episode 920, rewards -9.0\n",
      "Pong-v0, episode 930, rewards -13.0\n",
      "Pong-v0, episode 940, rewards -17.0\n",
      "Pong-v0, episode 950, rewards -11.0\n",
      "Pong-v0, episode 960, rewards -13.0\n",
      "Pong-v0, episode 970, rewards -6.0\n",
      "Pong-v0, episode 980, rewards -8.0\n",
      "Pong-v0, episode 990, rewards -10.0\n",
      "Pong-v0, episode 1000, rewards -7.0\n",
      "Pong-v0, episode 1010, rewards -11.0\n",
      "Pong-v0, episode 1020, rewards -13.0\n",
      "Pong-v0, episode 1030, rewards -8.0\n",
      "Pong-v0, episode 1040, rewards -9.0\n",
      "Pong-v0, episode 1050, rewards -13.0\n",
      "Pong-v0, episode 1060, rewards 1.0\n",
      "Pong-v0, episode 1070, rewards -12.0\n",
      "Pong-v0, episode 1080, rewards -10.0\n",
      "Pong-v0, episode 1090, rewards -10.0\n",
      "Pong-v0, episode 1100, rewards -1.0\n",
      "Pong-v0, episode 1110, rewards -10.0\n",
      "Pong-v0, episode 1120, rewards -13.0\n",
      "Pong-v0, episode 1130, rewards -12.0\n",
      "Pong-v0, episode 1140, rewards -10.0\n",
      "Pong-v0, episode 1150, rewards -7.0\n",
      "Pong-v0, episode 1160, rewards -10.0\n",
      "Pong-v0, episode 1170, rewards -10.0\n",
      "Pong-v0, episode 1180, rewards -13.0\n",
      "Pong-v0, episode 1190, rewards -10.0\n",
      "Pong-v0, episode 1200, rewards -15.0\n",
      "Pong-v0, episode 1210, rewards -7.0\n",
      "Pong-v0, episode 1220, rewards -10.0\n",
      "Pong-v0, episode 1230, rewards -7.0\n",
      "Pong-v0, episode 1240, rewards -9.0\n",
      "Pong-v0, episode 1250, rewards -12.0\n",
      "Pong-v0, episode 1260, rewards -9.0\n",
      "Pong-v0, episode 1270, rewards -8.0\n",
      "Pong-v0, episode 1280, rewards -15.0\n",
      "Pong-v0, episode 1290, rewards -11.0\n",
      "Pong-v0, episode 1300, rewards -10.0\n",
      "Pong-v0, episode 1310, rewards -9.0\n",
      "Pong-v0, episode 1320, rewards -10.0\n",
      "Pong-v0, episode 1330, rewards -9.0\n",
      "Pong-v0, episode 1340, rewards -5.0\n",
      "Pong-v0, episode 1350, rewards -2.0\n",
      "Pong-v0, episode 1360, rewards -14.0\n",
      "Pong-v0, episode 1370, rewards 2.0\n",
      "Pong-v0, episode 1380, rewards -13.0\n",
      "Pong-v0, episode 1390, rewards -11.0\n",
      "Pong-v0, episode 1400, rewards -8.0\n",
      "Pong-v0, episode 1410, rewards -3.0\n",
      "Pong-v0, episode 1420, rewards -11.0\n",
      "Pong-v0, episode 1430, rewards -5.0\n",
      "Pong-v0, episode 1440, rewards -12.0\n",
      "Pong-v0, episode 1450, rewards -6.0\n",
      "Pong-v0, episode 1460, rewards -9.0\n",
      "Pong-v0, episode 1470, rewards -8.0\n",
      "Pong-v0, episode 1480, rewards -11.0\n",
      "Pong-v0, episode 1490, rewards -6.0\n",
      "Pong-v0, episode 1500, rewards -6.0\n",
      "Pong-v0, episode 1510, rewards -9.0\n",
      "Pong-v0, episode 1520, rewards -9.0\n",
      "Pong-v0, episode 1530, rewards -5.0\n",
      "Pong-v0, episode 1540, rewards -13.0\n",
      "Pong-v0, episode 1550, rewards -6.0\n",
      "Pong-v0, episode 1560, rewards -6.0\n",
      "Pong-v0, episode 1570, rewards -8.0\n",
      "Pong-v0, episode 1580, rewards -12.0\n",
      "Pong-v0, episode 1590, rewards -11.0\n",
      "Pong-v0, episode 1600, rewards -15.0\n",
      "Pong-v0, episode 1610, rewards -7.0\n",
      "Pong-v0, episode 1620, rewards -11.0\n",
      "Pong-v0, episode 1630, rewards -9.0\n",
      "Pong-v0, episode 1640, rewards -15.0\n",
      "Pong-v0, episode 1650, rewards -9.0\n",
      "Pong-v0, episode 1660, rewards -2.0\n",
      "Pong-v0, episode 1670, rewards -8.0\n",
      "Pong-v0, episode 1680, rewards -8.0\n",
      "Pong-v0, episode 1690, rewards -9.0\n",
      "Pong-v0, episode 1700, rewards -4.0\n",
      "Pong-v0, episode 1710, rewards -6.0\n",
      "Pong-v0, episode 1720, rewards -8.0\n",
      "Pong-v0, episode 1730, rewards -2.0\n",
      "Pong-v0, episode 1740, rewards -10.0\n",
      "Pong-v0, episode 1750, rewards -6.0\n",
      "Pong-v0, episode 1760, rewards -2.0\n",
      "Pong-v0, episode 1770, rewards -10.0\n",
      "Pong-v0, episode 1780, rewards -10.0\n",
      "Pong-v0, episode 1790, rewards -10.0\n",
      "Pong-v0, episode 1800, rewards -5.0\n",
      "Pong-v0, episode 1810, rewards -11.0\n",
      "Pong-v0, episode 1820, rewards -8.0\n",
      "Pong-v0, episode 1830, rewards -3.0\n",
      "Pong-v0, episode 1840, rewards -10.0\n",
      "Pong-v0, episode 1850, rewards -10.0\n",
      "Pong-v0, episode 1860, rewards -9.0\n",
      "Pong-v0, episode 1870, rewards 5.0\n",
      "Pong-v0, episode 1880, rewards -11.0\n",
      "Pong-v0, episode 1890, rewards -9.0\n",
      "Pong-v0, episode 1900, rewards -9.0\n",
      "Pong-v0, episode 1910, rewards -6.0\n",
      "Pong-v0, episode 1920, rewards -5.0\n",
      "Pong-v0, episode 1930, rewards -9.0\n",
      "Pong-v0, episode 1940, rewards -11.0\n",
      "Pong-v0, episode 1950, rewards -3.0\n",
      "Pong-v0, episode 1960, rewards 5.0\n",
      "Pong-v0, episode 1970, rewards -8.0\n",
      "Pong-v0, episode 1980, rewards -8.0\n",
      "Pong-v0, episode 1990, rewards -7.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "RL_pong = Policy(6400, 2).to(device)\n",
    "rewards = []\n",
    "for episode in range(2000):\n",
    "    # Initiate one episode\n",
    "    observation, info = env.reset()\n",
    "    #observation = [item for sublist in observation for item in sublist]\n",
    "\n",
    "    obs_history = []\n",
    "    reward_history = []\n",
    "    action_history = []\n",
    "\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Roll out one episode\n",
    "    while (not terminated) and (not truncated):\n",
    "        #action = env.action_space.sample() # Use your policy here\n",
    "        observation = preprocess(observation)\n",
    "        observation = torch.from_numpy(observation).to(device)\n",
    "        p = RL_pong(observation)\n",
    "        action = RL_pong.choose_action(p) + 2\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        \n",
    "        #observation = [item for sublist in observation for item in sublist]\n",
    "        RL_pong.store_transition(observation, action-2, reward)\n",
    "        obs_history.append(observation)\n",
    "        reward_history.append(reward)\n",
    "        action_history.append(action)\n",
    "        \n",
    "        #print(\"observation\", np.shape(observation))\n",
    "    ep_rs_sum = sum(RL_pong.ep_rs)\n",
    "\n",
    "    if 'running_reward' not in globals():\n",
    "        running_reward = ep_rs_sum\n",
    "    else:\n",
    "        running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "    \n",
    "    rewards.append(ep_rs_sum)\n",
    "    vt = RL_pong.learn()\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Pong-v0, episode {episode}, rewards {ep_rs_sum}\")\n",
    "    if episode % 100 == 0:\n",
    "        np.save('./rewards.npy', rewards)\n",
    "    if episode % 500 == 0:\n",
    "        model_file_name = f\"model_episode_{episode}.pth\"\n",
    "        torch.save(RL_pong.state_dict(), model_file_name)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
