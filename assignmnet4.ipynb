{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7033ef9-9ea7-41b8-949b-8735dc0ede5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4917df6-e38d-41f4-a8cb-22b5abd4b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "trainX = np.load(\"./data/train_X.npy\")\n",
    "trainY = np.load(\"./data/train_y.npy\")\n",
    "testX = np.load(\"./data/test_X.npy\")\n",
    "testY = np.load(\"./data/test_y.npy\")\n",
    "\n",
    "def partition_data():\n",
    "    trainX_reshape = trainX.reshape(len(trainX),28*28)\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    trainX_reshape, trainY, test_size=0.2, random_state=42)\n",
    "    return [train_images, val_images, train_labels, val_labels]\n",
    "\n",
    "[x_train, x_val, y_train, y_val] = partition_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3cffde-8e05-42a2-9ecf-aed88784a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.layer1 = nn.Linear(28*28, 128)\n",
    "        self.layer2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        if self.activation==\"relu\":\n",
    "            x = torch.relu(x)\n",
    "        elif self.activation==\"sigmoid\":\n",
    "            x = torch.sigmoid(x)\n",
    "        elif self.activation==\"tanh\":\n",
    "            x = nn.Tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "def train_model(model, b=64, log_all=False):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    idxes = [i for i in range(len(y_train))]\n",
    "    shuffled_list = random.sample(idxes, len(idxes))\n",
    "    batch_idx = [shuffled_list[i:min(i+b,len(idxes))] for i in range(0, len(idxes), b)]\n",
    "    all_loss = []\n",
    "    all_f1 = []\n",
    "    for e in range(300):\n",
    "        total_correct = 0\n",
    "        total_loss = []\n",
    "        total_f1 = []\n",
    "        for b in range(len(batch_idx)):\n",
    "            optimizer.zero_grad()\n",
    "            bid = batch_idx[b]\n",
    "            batch_x = x_train[bid]\n",
    "            batch_y = y_train[bid]\n",
    "        \n",
    "            inputs = torch.FloatTensor(batch_x).to(device)\n",
    "            labels = torch.tensor(batch_y,dtype=torch.int64).to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            f1 = f1_score(labels_cpu,predicted,average=\"macro\")\n",
    "            total_f1.append(f1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "            #return loss.item(), total_correct/len(self.y_val), [self.model.layer1.weight, self.model.layer2.weight]\n",
    "        if e%50==0:\n",
    "            print(\"Training Loss: {l}, Training F1: {f}\".format(l=np.mean(total_loss),f=f1))\n",
    "            #validate_model(model, log_view=True)\n",
    "        all_loss.append(np.mean(total_loss))\n",
    "        all_f1.append(np.mean(total_f1))\n",
    "    val_loss, val_f1 = validate_model(model, log_view=False)\n",
    "    if log_all:\n",
    "        return all_loss, all_f1\n",
    "    return val_loss, val_f1\n",
    "    \n",
    "def validate_model(model, log_view = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.FloatTensor(x_val).to(device)\n",
    "        outputs = model(inputs) \n",
    "        labels = torch.tensor(y_val,dtype=torch.int64).to(device)\n",
    "        val_loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct = (predicted == labels).sum().item()\n",
    "        labels = labels.cpu().numpy()\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        f1 = f1_score(labels,predicted,average=\"macro\")\n",
    "        if log_view:\n",
    "            #print(\"total correct/ num_samples:\", total_correct, len(y_val))\n",
    "            print(\"Val Loss: {l}, Val F1 {f}\".format(l=val_loss.item(),f=f1))\n",
    "    return val_loss.item(), f1\n",
    "\n",
    "def test_model(model, log_view = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        testX_ = testX.reshape(len(testX),28*28)\n",
    "        inputs = torch.FloatTensor(testX_).to(device)\n",
    "        outputs = model(inputs) \n",
    "        labels = torch.tensor(testY,dtype=torch.int64).to(device)\n",
    "        val_loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct = (predicted == labels).sum().item()\n",
    "        f1 = f1_score(labels,predicted,average=\"macro\")\n",
    "        if log_view:\n",
    "            #print(\"total correct/ num_samples:\", total_correct, len(y_val))\n",
    "            print(\"Val Loss: {l}, Val F1 {f}\".format(l=val_loss.item(),f=f1))\n",
    "    return val_loss.item(), f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2d19f-3049-419a-8a22-c1aca36f493e",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe3de4b0-6638-4add-b17e-c689158da0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen2param(gen):\n",
    "    gen_p1 = gen[0:10]\n",
    "    gen_p2 = gen[10:13]\n",
    "    b = 0\n",
    "    for i in range(10):\n",
    "        b += gen_p1[i]*2**(9-i)\n",
    "    if b<16:\n",
    "        b = 16\n",
    "        gen[5] = 1\n",
    "        gen[6:10] = 0\n",
    "    if gen_p2[0] == 1:\n",
    "        activation = \"relu\"\n",
    "    elif gen_p2[1] == 1:\n",
    "        activation = \"sigmoid\"\n",
    "    elif gen_p2[2] == 1:\n",
    "        activation = \"tahn\"\n",
    "    return int(b), activation\n",
    "\n",
    "def cal_fitness(gen):\n",
    "    b, act = gen2param(gen)\n",
    "    model_gen = MLP2(activation=act).to(device)\n",
    "    l,f = train_model(model_gen, b)\n",
    "    return f\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    pos = np.random.randint(1,10)\n",
    "    child1 = np.zeros(13)\n",
    "    child2 = np.zeros(13)\n",
    "    child1[0:pos] = parent1[0:pos].copy()\n",
    "    child1[pos+1:13] = parent2[pos+1:13].copy()\n",
    "    child2[0:pos] = parent2[0:pos].copy()\n",
    "    child2[pos+1:13] = parent1[pos+1:13].copy()\n",
    "    return child1, child2\n",
    "\n",
    "def mutation(parent1, parent2):\n",
    "    pos1 = np.random.randint(0,10) \n",
    "    pos2 = np.random.randint(0,10)\n",
    "    child1 = parent1.copy()\n",
    "    child2 = parent2.copy()\n",
    "    if child1[pos1] == 0:\n",
    "        child1[pos1] = 1\n",
    "    else:\n",
    "        child1[pos1] = 0\n",
    "    if child2[pos1] == 0:\n",
    "        child2[pos1] = 1\n",
    "    else:\n",
    "        child2[pos1] = 0\n",
    "    return child1, child2\n",
    "\n",
    "class Population():\n",
    "    def __init__(self):\n",
    "        g1 = np.array([1,1,0,0,0,0,0,0,0,0,1,0,0])\n",
    "        g2 = np.array([0,0,1,1,0,0,0,0,0,0,1,0,0])\n",
    "        g3 = np.array([0,0,0,0,1,1,0,0,0,0,1,0,0])\n",
    "        g4 = np.array([0,0,0,0,1,1,0,0,0,0,0,1,0])\n",
    "        g5 = np.array([0,0,0,0,1,1,0,0,0,0,0,0,1])\n",
    "        f1 = cal_fitness(g1)\n",
    "        f2 = cal_fitness(g2)\n",
    "        f3 = cal_fitness(g3)\n",
    "        f4 = cal_fitness(g4)\n",
    "        f5 = cal_fitness(g5)\n",
    "        population = [[g1,1,f1], [g2,1,f2],[g3,1,f3],\n",
    "                      [g4,1,f4],[g5,1,f5]]\n",
    "        self.fit_hist = [(f1+f2+f3+f4+f5)/5]\n",
    "        self.fit_best = [max(f1,f2,f3,f4,f5)]\n",
    "        self.population = population\n",
    "        \n",
    "    def Roulette(self):\n",
    "        pop_num = len(self.population)\n",
    "        fitness = [self.population[i][2] for i in range(pop_num)]\n",
    "        prob = softmax(fitness)\n",
    "        idx = np.random.choice([i for i in range(pop_num)],2,replace=False,p=prob)\n",
    "        idx1, idx2 = idx[0], idx[1]\n",
    "        self.population[idx1][1] += 1\n",
    "        self.population[idx2][1] += 1\n",
    "        return self.population[idx1][0], self.population[idx2][0]\n",
    "    \n",
    "    def add_child(self, ch1, ch2):\n",
    "        f1 = cal_fitness(ch1)\n",
    "        f2 = cal_fitness(ch2)\n",
    "        self.population.append([ch1,1,f1])\n",
    "        self.population.append([ch2,1,f2])\n",
    "        pop_num = len(self.population)\n",
    "        fitness = [self.population[i][2] for i in range(pop_num)]\n",
    "        fitness_new = np.mean(fitness)\n",
    "        fitness_best = np.max(fitness)\n",
    "        self.fit_hist.append(fitness_new)\n",
    "        self.fit_best.append(fitness_best)\n",
    "        np.save(\"./fitness.npy\", self.fit_hist)\n",
    "        np.save(\"./best_hist.npy\", self.fit_best)\n",
    "        return fitness_new\n",
    "        \n",
    "    def del_parent(self):\n",
    "        i = 0\n",
    "        while i < len(self.population):\n",
    "            if self.population[i][1] > 5:\n",
    "                del self.population[i]\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    def best_gene(self):\n",
    "        pop_num = len(self.population)\n",
    "        fitness = [self.population[i][2] for i in range(pop_num)]\n",
    "        best_idx = fitness.index(max(fitness))\n",
    "        best_gene = self.population[best_idx]\n",
    "        np.save(\"./fitness.npy\", self.fit_hist)\n",
    "        return best_gene\n",
    "        \n",
    "\n",
    "def geneticAlg():\n",
    "    # initialization\n",
    "    pop = Population()\n",
    "    for i in range(300):\n",
    "        # Roulette\n",
    "        pr1, pr2 = pop.Roulette()\n",
    "        # crosssover\n",
    "        if np.random.rand()>0.1:\n",
    "            ch1, ch2 = crossover(pr1,pr2)\n",
    "        # mutation\n",
    "        else:\n",
    "            ch1, ch2 = mutation(pr1,pr2)\n",
    "        # calculate fitness\n",
    "        fit_i = pop.add_child(ch1, ch2)\n",
    "        pop.del_parent()\n",
    "        print(i,fit_i)\n",
    "    best_gen = pop.best_gene()\n",
    "    return best_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee6b96c-3da4-4699-97bf-c11ec588169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7455535443357276\n",
      "1 0.7807808639155283\n",
      "2 0.7830926040089342\n",
      "3 0.7938524733765722\n",
      "4 0.7985307798070495\n",
      "5 0.7589750076158356\n",
      "6 0.7737703953537597\n",
      "7 0.7852639044667687\n",
      "8 0.7929053840438418\n",
      "9 0.7866772058410478\n",
      "10 0.7880471370206088\n",
      "11 0.7931909659638579\n",
      "12 0.7993889860184556\n",
      "13 0.8028495956393567\n",
      "14 0.8056804000816353\n",
      "15 0.8101874499366113\n",
      "16 0.8141067602198812\n",
      "17 0.8181779710151348\n",
      "18 0.8176405753884567\n",
      "19 0.8172323494602965\n",
      "20 0.8207933515927662\n",
      "21 0.8213349988062223\n",
      "22 0.8233183596703065\n",
      "23 0.8230366067616135\n",
      "24 0.8250719240379517\n",
      "25 0.8273612717010844\n",
      "26 0.8257049058364464\n",
      "27 0.8282058294726782\n",
      "28 0.8260622141948292\n",
      "29 0.8264023999803298\n",
      "30 0.8174703356164874\n",
      "31 0.8194338166136981\n",
      "32 0.8212513284346865\n",
      "33 0.8228075009284775\n",
      "34 0.8222610121351039\n",
      "35 0.8244341866904861\n",
      "36 0.8258335895257858\n",
      "37 0.8269307120947573\n",
      "38 0.8289983184422751\n",
      "39 0.8285651597977416\n",
      "40 0.8302102468714903\n",
      "41 0.8313573861659258\n",
      "42 0.8328605549343998\n",
      "43 0.8344317616240396\n",
      "44 0.8360608695103672\n",
      "45 0.8368554883324206\n",
      "46 0.8376434106987681\n",
      "47 0.8392487822557295\n",
      "48 0.8405568894361125\n",
      "49 0.8416248956904503\n",
      "50 0.8429711164113508\n",
      "51 0.8418232743685903\n",
      "52 0.8428655084598423\n",
      "53 0.8372309347775736\n",
      "54 0.8382522710549525\n",
      "55 0.8389392595434209\n",
      "56 0.8395829653067364\n",
      "57 0.839837245589913\n",
      "58 0.8407648423010857\n",
      "59 0.8406940154185714\n",
      "60 0.8411523568506936\n",
      "61 0.8355504657230773\n",
      "62 0.8361595292151763\n",
      "63 0.8371834577122392\n",
      "64 0.837106494182995\n",
      "65 0.8365418736125378\n",
      "66 0.8376041409777166\n",
      "67 0.8373200480828046\n",
      "68 0.8368684961301232\n",
      "69 0.8375753327432032\n",
      "70 0.8361545365359186\n",
      "71 0.8366377831342741\n",
      "72 0.8374634537494503\n",
      "73 0.8376965003416557\n",
      "74 0.8384741308690704\n",
      "75 0.8388775077814623\n",
      "76 0.8397628143622405\n",
      "77 0.8399816987818324\n",
      "78 0.8409262364574495\n",
      "79 0.8415583430278859\n",
      "80 0.8415797467956851\n",
      "81 0.8404635632985692\n",
      "82 0.8409987359760881\n",
      "83 0.8411909474372643\n",
      "84 0.8413849306650841\n",
      "85 0.8417034770983448\n",
      "86 0.8421833821813666\n",
      "87 0.8427503719973223\n",
      "88 0.8431175766556006\n",
      "89 0.8426237572790255\n",
      "90 0.8432558107361863\n",
      "91 0.8434946010168187\n",
      "92 0.8441317940541812\n",
      "93 0.8447286017166875\n",
      "94 0.84515636251779\n",
      "95 0.8452989629110678\n",
      "96 0.8451700862442103\n",
      "97 0.8446618999524703\n",
      "99 0.8455918252134056\n",
      "101 0.846566452574357\n",
      "103 0.8475082866404777\n",
      "104 0.8471561415917033\n",
      "105 0.8470272916381102\n",
      "106 0.8474502014791359\n",
      "107 0.8474841753693231\n",
      "108 0.8479134086401976\n",
      "109 0.848299659963069\n",
      "110 0.8485178933382596\n",
      "111 0.8481305207925347\n",
      "112 0.8486382453249964\n",
      "113 0.8489034692696371\n",
      "114 0.8493893546030665\n",
      "115 0.8497004702269769\n",
      "116 0.8499443787158607\n",
      "117 0.8501681395822923\n",
      "118 0.8501331473516055\n",
      "119 0.8505821570317129\n",
      "120 0.850821226524806\n",
      "121 0.8511791246700875\n",
      "122 0.8512947900706809\n",
      "123 0.8518010047598508\n",
      "124 0.8513971904539069\n",
      "125 0.85212977966978\n",
      "126 0.8524252859571194\n",
      "127 0.8526323826565277\n",
      "128 0.852698163204381\n",
      "129 0.8515990134847942\n",
      "130 0.8517819340188311\n",
      "131 0.8518900300547572\n",
      "132 0.8495326367944742\n",
      "133 0.8491872649133221\n",
      "134 0.8488550843450954\n",
      "135 0.8492801416530084\n",
      "136 0.8494273519641511\n",
      "137 0.849922484673879\n",
      "138 0.8503246747044556\n",
      "139 0.8505262940598614\n",
      "140 0.8483609719143096\n",
      "141 0.8479886640519473\n",
      "142 0.848730229592777\n",
      "143 0.8489069367929355\n",
      "144 0.8490709768783355\n",
      "145 0.8494580865557598\n",
      "146 0.8498956136313988\n",
      "147 0.8501890601540207\n",
      "148 0.850406068173645\n",
      "149 0.8506922920456027\n",
      "150 0.8506081963187334\n",
      "151 0.8489998322582923\n",
      "152 0.849290304388723\n",
      "153 0.8496255606012816\n",
      "154 0.850029424664662\n",
      "156 0.8507784865727321\n",
      "157 0.8510398898874032\n",
      "158 0.8513250499127302\n",
      "159 0.8511201503204908\n",
      "160 0.8513006161771728\n",
      "161 0.8516831307352907\n",
      "162 0.8518843685084653\n",
      "163 0.8521088351233145\n",
      "164 0.8524891507759706\n",
      "165 0.852611963093189\n",
      "166 0.8528825695787732\n",
      "167 0.8529828711882894\n",
      "168 0.8531559402582056\n",
      "169 0.8528433384587388\n",
      "170 0.8531202887668431\n",
      "171 0.853379310229193\n",
      "172 0.8534381344988566\n",
      "173 0.8536124280847381\n",
      "174 0.8539835786462623\n",
      "176 0.8539792716624709\n",
      "177 0.8540521409563814\n",
      "178 0.8541538729446269\n",
      "179 0.8538127827813405\n",
      "180 0.8540401018750489\n",
      "181 0.8540364010305903\n",
      "184 0.8544297733416215\n",
      "185 0.8547487051946481\n",
      "186 0.8548021271355482\n",
      "187 0.8549409125766538\n",
      "188 0.8539481643982052\n",
      "189 0.854105164298399\n",
      "190 0.8532919228306185\n",
      "191 0.8531483419043712\n",
      "192 0.8533922298953094\n",
      "193 0.8534759958172775\n",
      "194 0.8537721352341587\n",
      "195 0.8540468528862816\n",
      "196 0.8542207504290177\n",
      "197 0.8543850794627881\n",
      "198 0.8545633634687066\n",
      "199 0.8548215756326607\n",
      "200 0.8545985496431825\n",
      "201 0.8542907001296347\n",
      "202 0.8535953236298282\n",
      "203 0.8538111315226076\n",
      "204 0.8540830581222246\n",
      "205 0.8542097991053644\n",
      "206 0.8543841185417782\n",
      "207 0.854590376406878\n",
      "208 0.8546588556367415\n",
      "209 0.8546907049061503\n",
      "210 0.8548434881024003\n",
      "211 0.8549979397515439\n",
      "212 0.8550610214122419\n",
      "213 0.8551402845630022\n",
      "214 0.8551050790198002\n",
      "215 0.8550679245049886\n",
      "216 0.8552184925787029\n",
      "217 0.8553960937581274\n",
      "218 0.8556001081900448\n",
      "219 0.8556431003474169\n",
      "220 0.8557134967175976\n",
      "221 0.8559994231333751\n",
      "222 0.8560601293456126\n",
      "223 0.8559437444586016\n",
      "224 0.8558828417374121\n",
      "225 0.856041461244328\n",
      "226 0.8560105185417565\n",
      "227 0.8562788238423101\n",
      "228 0.8563030099898121\n",
      "229 0.8565161182755278\n",
      "230 0.856545535607471\n",
      "231 0.8563874489427079\n",
      "232 0.8565150596578299\n",
      "233 0.8565880421314553\n",
      "234 0.8567507290837684\n",
      "235 0.8566198505091844\n",
      "236 0.8565651084217767\n",
      "237 0.8567675319805272\n",
      "238 0.8569164975576885\n",
      "239 0.8570401925802204\n",
      "240 0.8570986334333878\n",
      "241 0.8573083968119429\n",
      "242 0.857473537690519\n",
      "243 0.857703835272422\n",
      "244 0.8578405687723721\n",
      "245 0.857529961708501\n",
      "246 0.8576007129765146\n",
      "247 0.8577246432278046\n",
      "248 0.8576449174952461\n",
      "249 0.8578079154309781\n",
      "250 0.8578939362162188\n",
      "251 0.8579947077864007\n",
      "252 0.8582185810760887\n",
      "253 0.8584044697634267\n",
      "254 0.8584875288656773\n",
      "255 0.8586472092232833\n",
      "256 0.8587819211622529\n",
      "257 0.8589168589527106\n",
      "258 0.8587682076710587\n",
      "259 0.8588435568272227\n",
      "260 0.858617747720348\n",
      "261 0.8586812256092966\n",
      "262 0.8584671171941731\n",
      "263 0.8585477490369153\n",
      "264 0.8587066736913821\n",
      "265 0.8588165937745647\n",
      "266 0.858688985685399\n",
      "267 0.8573599682668005\n",
      "268 0.8575400501317801\n",
      "269 0.8575408704365575\n",
      "270 0.8573806941884736\n",
      "271 0.8574433093422434\n",
      "272 0.8568190073444539\n",
      "273 0.8570256294934033\n",
      "274 0.8571358301772729\n",
      "275 0.8573406377539474\n",
      "276 0.8575446937323442\n",
      "277 0.8574481933347925\n",
      "278 0.857528137589541\n",
      "279 0.8576341375527742\n",
      "280 0.8562887524410011\n",
      "281 0.8565339625819418\n",
      "282 0.8567074116638334\n",
      "283 0.8567959795539811\n",
      "284 0.8568139180267024\n",
      "285 0.8569770786583016\n",
      "286 0.8571124403941922\n",
      "287 0.8572482545647007\n",
      "288 0.8574162213695741\n",
      "289 0.8574800780035712\n",
      "290 0.8576131871282877\n",
      "291 0.8574813800422151\n",
      "292 0.8574843050844391\n",
      "293 0.8576032984260112\n",
      "294 0.8577685637054628\n",
      "295 0.8578871572017293\n",
      "296 0.8580057567001138\n",
      "297 0.8581066688851458\n",
      "298 0.857808743899729\n",
      "299 0.8577318135294782\n"
     ]
    }
   ],
   "source": [
    "best_gen = geneticAlg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012509ec-f127-4696-85f0-b4920840af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.]), 1, 0.9356107917861086]\n"
     ]
    }
   ],
   "source": [
    "print(best_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e746150-7252-41fb-ab4a-574243562c0f",
   "metadata": {},
   "source": [
    "The best combination of hyperparameters are: b = 16, activation function is ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75034142-ea79-4509-a2dd-4ced5bc9066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = MLP2(activation=\"relu\").to(device)\n",
    "l,f = train_model(model_best, b=16, log_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08766b9e-f12d-47a6-a248-7182a0d0a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(f, label=\"F_1 Score\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Training F_1')\n",
    "plt.title('Training F_1 Score V.S. Generations')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265db52-acb6-492a-9ada-6683366d7204",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d98846-bc47-4303-b316-2c0ee72a3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the black box function for Bayesian Optimization\n",
    "def black_box_func(b,act):\n",
    "    if act == 0:\n",
    "        model = MLP2(activation=\"relu\")\n",
    "    elif act == 1:\n",
    "        model = MLP2(activation=\"sigmoid\")\n",
    "    elif act == 2:\n",
    "        model = MLP2(activation=\"tanh\")\n",
    "    l, f = train_model(model, b)\n",
    "    return f\n",
    "\n",
    "def f_opt(b,act):\n",
    "    act = int(act)\n",
    "    b = int(b)\n",
    "    f = black_box_func(b,act)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c479058-14cb-4cc9-8f08-44b21fddca60",
   "metadata": {},
   "outputs": [],
   "source": [
    " from bayes_opt import BayesianOptimization\n",
    "optimizer = BayesianOptimization(f=f_opt,\n",
    "                                 pbounds={'b':(16,1024),'act':(0,2.99)},\n",
    "                                 verbose=2,\n",
    "                                 random_state=1,\n",
    "                                 allow_duplicate_points=True)\n",
    "\n",
    "optimizer.set_gp_params(alpha=1e-3)\n",
    "optimizer.maximize(n_iter=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa07fcd-74ed-46a4-b4a8-b90c59b4d461",
   "metadata": {},
   "source": [
    "The best combination of hyperparameters are: b=17, activation function is tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd901e-656b-439d-90d8-2c3cfb15ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = MLP2(activation=\"tahn\").to(device)\n",
    "l,f = train_model(model_best, b=17, log_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae1a2a-020c-4759-aafa-bc4627744070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(f, label=\"F_1 Score\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Training F_1')\n",
    "plt.title('Training F_1 Score V.S. Generations')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iems469-xdl",
   "language": "python",
   "name": "iems469-xdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
